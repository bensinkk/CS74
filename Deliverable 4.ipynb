{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe loading, processing, and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    text = ' '.join(str(text).split())\n",
    "    return text\n",
    "\n",
    "# Iterator to define a product as \"awesome\" or not based on its overall rating (>4.5 threshold for Sports)\n",
    "def df_iter(overall):\n",
    "    if overall > 4.5:\n",
    "        result = 'awesome'\n",
    "    else:\n",
    "        result = 'not'\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes the JSON dataset and returns a dataframe wherein products are grouped by their ID (ASIN), \n",
    "# & associated with their overall review rating, from which we know their \"awesomeness\" or not.\n",
    "def load_process_file(filename):\n",
    "    df = pd.read_json(filename, lines=True)\n",
    "    \n",
    "    grouped_df = df.groupby(\"asin\")\n",
    "    grouped_lists = grouped_df[['summary', 'reviewText']].apply(text_process).reset_index()\n",
    "    \n",
    "    mean_df = grouped_df['overall'].mean()\n",
    "    mean_df = mean_df.reset_index()\n",
    "    \n",
    "    final_df = pd.merge(grouped_lists, mean_df, on=\"asin\")\n",
    "    final_df['class'] = final_df.apply(lambda row: df_iter(row['overall']), axis=1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns X_train, X_test, y_train, y_test\n",
    "def vectorize(final_df):\n",
    "    \n",
    "    # stemming, stop words dictionary, tokenizer\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    \n",
    "    # Feature extraction with CountVectorizer. (Tried TFIDF, didn't work as well.)\n",
    "    # ngrame range shortened from trigrams back to bigrams: improves performance & time-efficiency\n",
    "    cv = CountVectorizer(preprocessor=stemmer.stem, stop_words=stop_words, ngram_range = (1,2), tokenizer=token.tokenize)\n",
    "    text_counts = cv.fit_transform(final_df[0])\n",
    "    \n",
    "    # Generate & return\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_counts, final_df['class'], test_size = 0.25, random_state = 5)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: AdaBoost w/ Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost LinearSVC\n",
    "def boost_svc(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    boost_classifier = AdaBoostClassifier(LinearSVC(loss='squared_hinge'),\n",
    "                                          algorithm='SAMME',\n",
    "                                          learning_rate=0.125,\n",
    "                                          n_estimators=200)\n",
    "    \n",
    "    boost_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_score = boost_classifier.decision_function(X_test)\n",
    "    y_pred_boost = boost_classifier.predict(X_test)\n",
    "    f1_score_boost = metrics.f1_score(y_test, y_pred_boost, average='weighted')\n",
    "    precision_score_boost = metrics.average_precision_score(y_test, y_score, average='weighted', pos_label=\"not\")\n",
    "\n",
    "    print('Boosted SVC F1: ' + str('{:04.2f}'.format(f1_score_boost*100)) + '%')\n",
    "    print('Boosted SVC Precision-Recall: ' + str('{:04.2f}'.format(precision_score_boost*100)) + '%')\n",
    "    \n",
    "    return boost_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-boosted Logistic Regression\n",
    "def log_reg(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    logreg = LogisticRegression(penalty='l2',\n",
    "                                solver='liblinear',\n",
    "                                max_iter=10000,\n",
    "                                class_weight='balanced',\n",
    "                                tol=1e-2,\n",
    "                                verbose=True)\n",
    "    \n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the results, calculating accuracy\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print(\"Accuracy of logistic regression classifier on test set: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "    \n",
    "    # Compute F-1, precision, recall\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print('Logistic regression F1: ' + str('{:04.2f}'.format(f1_score*100)) + '%')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    y_score = logreg.decision_function(X_test)\n",
    "    precision_score_lr = metrics.average_precision_score(y_test, y_score, average='weighted', pos_label=\"not\")\n",
    "    print('LR Precision-Recall: ' + str('{:04.2f}'.format(precision_score_lr*100)) + '%')\n",
    "    \n",
    "    return log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: AdaBoost w/ Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Naive Bayes\n",
    "def boost_NB(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    boost_nb = AdaBoostClassifier(base_estimator=ComplementNB(),\n",
    "                                      n_estimators=1000,\n",
    "                                      learning_rate=0.1)\n",
    "    boost_nb.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = boost_nb.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, predicted, average='weighted')\n",
    "    \n",
    "    print('F1: ' + str('{:04.2f}'.format(f1_score*100)) + '%')\n",
    "    \n",
    "    return boost_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: Voting w/ AdaBoost'd LinearSVC & Logistic Regression as estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_hard(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    boostsvc = AdaBoostClassifier(LinearSVC(loss='squared_hinge'),\n",
    "                                          algorithm='SAMME',\n",
    "                                          learning_rate=0.125,\n",
    "                                          n_estimators=200)\n",
    "    \n",
    "    logreg = LogisticRegression(penalty='l2',\n",
    "                                solver='saga',\n",
    "                                max_iter=10000,\n",
    "                                class_weight='balanced')\n",
    "    \n",
    "    vc_hard = VotingClassifier(estimators=[('svc',boostsvc),('lr',logreg)],\n",
    "                          voting='hard',\n",
    "                          n_jobs=1)\n",
    "    \n",
    "    vc_hard.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred_vc_hard = vc_hard.predict(X_test)\n",
    "    f1_score_vc_hard = metrics.f1_score(y_test, y_pred_vc_hard, average='weighted')\n",
    "    \n",
    "    print('F1 (VC hard): ' + str('{:04.2f}'.format(f1_score_vc_hard*100)) + '%')\n",
    "    print(metrics.classification_report(y_test, y_pred_vc_hard))\n",
    "    \n",
    "    return vc_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/Sports_and_Outdoors_Reviews_training.json'\n",
    "final_df = load_process_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = vectorize(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = voting_hard(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions on test dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
