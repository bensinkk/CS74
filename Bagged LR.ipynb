{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    text = ' '.join(str(text).split())\n",
    "    return text\n",
    "\n",
    "# Iterator to define a product as \"awesome\" or not based on its overall rating (>4.5 threshold for Sports)\n",
    "def df_iter(overall):\n",
    "    if overall > 4.5:\n",
    "        result = 'awesome'\n",
    "    else:\n",
    "        result = 'not'\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes the JSON dataset and returns a dataframe wherein products are grouped by their ID (ASIN), \n",
    "# & associated with their overall review rating, from which we know their \"awesomeness\" or not.\n",
    "def load_process_file():\n",
    "    df = pd.read_json('data/Sports_and_Outdoors_Reviews_training.json', lines=True)\n",
    "    \n",
    "    grouped_df = df.groupby(\"asin\")\n",
    "    grouped_lists = grouped_df[['summary', 'reviewText']].apply(text_process).reset_index()\n",
    "    \n",
    "    mean_df = grouped_df['overall'].mean()\n",
    "    mean_df = mean_df.reset_index()\n",
    "    \n",
    "    final_df = pd.merge(grouped_lists, mean_df, on=\"asin\")\n",
    "    final_df['class'] = final_df.apply(lambda row: df_iter(row['overall']), axis=1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns X_train, X_test, y_train, y_test\n",
    "def vectorize(final_df):\n",
    "    \n",
    "    # stemming, stop words dictionary, tokenizer\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    \n",
    "    # Feature extraction with CountVectorizer. (Tried TFIDF, didn't work as well.)\n",
    "    # ngrame range shortened from trigrams back to bigrams: improves performance & time-efficiency\n",
    "    cv = CountVectorizer(preprocessor=stemmer.stem, stop_words=stop_words, ngram_range = (1,2), tokenizer=token.tokenize)\n",
    "    text_counts = cv.fit_transform(final_df[0])\n",
    "    \n",
    "    # Generate & return\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_counts, final_df['class'], test_size = 0.25, random_state = 5)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost LinearSVC â€” doesn't always improve performance.\n",
    "def boost_svc(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    boost_classifier = AdaBoostClassifier(LinearSVC(loss='squared_hinge'),\n",
    "                                          algorithm='SAMME',\n",
    "                                          learning_rate=0.125,\n",
    "                                          n_estimators=200)\n",
    "    \n",
    "    boost_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_score = boost_classifier.decision_function(X_test)\n",
    "    y_pred_boost = boost_classifier.predict(X_test)\n",
    "    f1_score_boost = metrics.f1_score(y_test, y_pred_boost, average='weighted')\n",
    "    precision_score_boost = metrics.average_precision_score(y_test, y_score, average='weighted', pos_label=\"not\")\n",
    "\n",
    "    print('Boosted SVC F1: ' + str('{:04.2f}'.format(f1_score_boost*100)) + '%')\n",
    "    print('Boosted SVC Precision-Recall: ' + str('{:04.2f}'.format(precision_score_boost*100)) + '%')\n",
    "    \n",
    "    return boost_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Non-boosted Logistic Regression\n",
    "def log_reg(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    logreg = LogisticRegression(penalty='l2',\n",
    "                                solver='saga',\n",
    "                                max_iter=10000,\n",
    "                                class_weight='balanced',\n",
    "                                tol=1e-2,\n",
    "                                verbose=True)\n",
    "    \n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the results, calculating accuracy\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print(\"Accuracy of logistic regression classifier on test set: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "    \n",
    "    # Compute F-1, precision, recall\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print('Logistic regression F1: ' + str('{:04.2f}'.format(f1_score*100)) + '%')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    y_score = logreg.decision_function(X_test)\n",
    "    precision_score_lr = metrics.average_precision_score(y_test, y_score, average='weighted', pos_label=\"not\")\n",
    "    print('LR Precision-Recall: ' + str('{:04.2f}'.format(precision_score_lr*100)) + '%')\n",
    "    \n",
    "    return log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Naive Bayes\n",
    "def boost_NB(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    boost_nb = AdaBoostClassifier(base_estimator=ComplementNB(),\n",
    "                                      n_estimators=1000,\n",
    "                                      learning_rate=0.1)\n",
    "    boost_nb.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = boost_nb.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, predicted, average='weighted')\n",
    "    \n",
    "    print('F1: ' + str('{:04.2f}'.format(f1_score*100)) + '%')\n",
    "    \n",
    "    return boost_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_hard(X_train, X_test, y_train, y_test, classifier1, classifier2):\n",
    "\n",
    "    boostsvc = AdaBoostClassifier(LinearSVC(loss='squared_hinge'),\n",
    "                                          algorithm='SAMME',\n",
    "                                          learning_rate=0.125,\n",
    "                                          n_estimators=200)\n",
    "    \n",
    "    logreg = LogisticRegression(penalty='l2',\n",
    "                                solver='saga',\n",
    "                                max_iter=10000,\n",
    "                                class_weight='balanced')\n",
    "    \n",
    "    vc_hard = VotingClassifier(estimators=[('svc',classifier1),('lr',classifier2)],\n",
    "                          voting='hard',\n",
    "                          n_jobs=1,\n",
    "                          verbose=True)\n",
    "    \n",
    "    vc_hard.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred_vc_hard = vc_hard.predict(X_test)\n",
    "    f1_score_vc_hard = metrics.f1_score(y_test, y_pred_vc_hard, average='weighted')\n",
    "    \n",
    "    print('F1 (VC hard): ' + str('{:04.2f}'.format(f1_score_vc_hard*100)) + '%')\n",
    "    print(metrics.classification_report(y_test, y_pred_vc_hard))\n",
    "    \n",
    "    return vc_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier1=boostsvc, classifier2=logreg\n",
    "def voting_soft(X_train, X_test, y_train, y_test, classifier1, classifier2):\n",
    "\n",
    "    vc_soft = VotingClassifier(estimators=[('svc', classifier1),('lr', classifier2)],\n",
    "                          voting='soft',\n",
    "                          n_jobs=1,\n",
    "                          verbose=True)\n",
    "    \n",
    "    vc_soft.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_vc_soft = vc_soft.predict(X_test)\n",
    "    f1_score_vc_soft = metrics.f1_score(y_test, y_pred_vc_soft, average='weighted')\n",
    "    \n",
    "    print(\"F1 (VC soft): \" + str('{:04.2f}'.format(f1_score_vc_soft*100)) + '%')\n",
    "    print(metrics.classification_report(y_test, y_pred_vc_soft))\n",
    "    \n",
    "    return vc_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search(X_train, X_test, y_train, y_test, vc):\n",
    "\n",
    "    params = {'svc__n_estimators': [20, 200], 'lr__C': [1.0, 100.0]}\n",
    "\n",
    "    # 5-fold cross validation\n",
    "    grid = GridSearchCV(estimator=vc, param_grid=params, cv=5)\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_grid = grid.predict(X_test)\n",
    "    f1_score_grid = metrics.f1_score(y_test, y_pred_grid, average='weighted')\n",
    "    print(\"F1 (GridSearchCV): \" + str('{:04.2f}'.format(f1_score_grid*100)) + '%')\n",
    "    print(metrics.classification_report(y_test, y_pred_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = load_process_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = vectorize(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosted_naive_bayes = boost_NB(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted SVC F1: 78.54%\n",
      "Boosted SVC Precision-Recall: 89.88%\n"
     ]
    }
   ],
   "source": [
    "boosted_svc = boost_svc(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 30 epochs took 28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.78\n",
      "Logistic regression F1: 78.22%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     awesome       0.73      0.80      0.77      9304\n",
      "         not       0.83      0.76      0.80     11634\n",
      "\n",
      "    accuracy                           0.78     20938\n",
      "   macro avg       0.78      0.78      0.78     20938\n",
      "weighted avg       0.79      0.78      0.78     20938\n",
      "\n",
      "LR Precision-Recall: 89.31%\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = log_reg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d9a0a49dbc96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoting_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboosted_svc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-dd93cef6027c>\u001b[0m in \u001b[0;36mvoting_hard\u001b[0;34m(X_train, X_test, y_train, y_test, classifier1, classifier2)\u001b[0m\n\u001b[1;32m     14\u001b[0m                           \u001b[0mvoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hard'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                           \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                           verbose=True)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mvc_hard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "vc = voting_hard(X_train, X_test, y_train, y_test, boosted_svc, logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(X_train, X_test, y_train, y_test, vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aight looks like it's time to **boost** LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging with logistic regression\n",
    "# estimators = [50,100,150,200,250,300]\n",
    "# samples = [0.1,0.2,0.3,0.4,0.5]\n",
    "# for e in estimators:\n",
    "#     for s in samples:\n",
    "#         print('estimators = {}\\nsamples = {}'.format(e,s))\n",
    "#         bag_classifier = BaggingClassifier(base_estimator=LogisticRegression(penalty='l2',\n",
    "#                                                                              solver='saga',\n",
    "#                                                                              max_iter=10000,\n",
    "#                                                                              tol=1e-2,\n",
    "#                                                                              class_weight='balanced'),\n",
    "#                                            max_samples=s,\n",
    "#                                            n_estimators=e,\n",
    "#                                            verbose=1,\n",
    "#                                            n_jobs=-1)\n",
    "#         bag_classifier.fit(X_train, y_train)\n",
    "#         y_score = bag_classifier.decision_function(X_test)\n",
    "#         y_pred_bag = bag_classifier.predict(X_test)\n",
    "#         f1_score = metrics.f1_score(y_test, y_pred_bag, average='weighted')\n",
    "#         print('F1: ' + str('{:04.2f}'.format(f1_score*100)) + '%')\n",
    "#         print(metrics.classification_report(y_test, y_pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score = metrics.f1_score(y_test, y_pred_bag, average='weighted')\n",
    "# print('F1: ' + str('{:04.2f}'.format(f1_score*100)) + '%')\n",
    "# print(metrics.classification_report(y_test, y_pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting with logistic regression\n",
    "# boo = AdaBoostClassifier(base_estimator=LogisticRegression(penalty='l2',\n",
    "#                                                            solver='saga',\n",
    "#                                                            max_iter=10000,\n",
    "#                                                            tol=1e-2,\n",
    "#                                                            class_weight='balanced'),\n",
    "#                          learning_rate=0.4,\n",
    "#                          n_estimators=100,\n",
    "#                          algorithm='SAMME')\n",
    "# boo.fit(X_train, y_train)\n",
    "# y_score = boo.decision_function(X_test)\n",
    "# y_pred_boo = boo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score = metrics.f1_score(y_test, y_pred_boo, average='weighted')\n",
    "# print('F1: ' + str('{:04.2f}'.format(f1_score*100)) + '%')\n",
    "# print(metrics.classification_report(y_test, y_pred_boo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
